{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suffolk Snow Day?\n",
    "\n",
    "I was asked to [talk](https://docs.google.com/presentation/d/1Kknr2pmhomVRHV1-6bJG2pGfJytQqGJ3o8FL_yGqp9s/edit?usp=sharing) to a group of law students and to put together a hands-on activity to give them a feel for what I do as a data scientist. So I came up with this grossly simplified workflow for answering that age-old question, \"Can we predict if the law school will call a snow day?\"\n",
    "\n",
    "How? That's simple, **statistics**, or as many like to call it these days, AI/machine learning. So let's get going.\n",
    "\n",
    "## Load Some Stuff\n",
    "\n",
    "To run this code, [download, install, and run Notebooks](https://github.com/colarusso/measured_justice#download-install-and-run-notebooks). After that, [download](https://github.com/colarusso/regression_demo/archive/master.zip) and unzip this project. FYI, you'll have to load a few modules. So you may want to check out these [notes](https://github.com/colarusso/measured_justice#common-speed-bumps) on common speed bumps. Anyhow, the following code loads the tools we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcolarusso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    inputFunc = raw_input\n",
    "except NameError:\n",
    "    inputFunc = input\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "import numpy as np\n",
    "  \n",
    "#import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import re \n",
    "import json    \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "We want to build a model that will take in some data and spit out a prediction. So we'll need some training data. Specificly, if we think snow closures are based on weather conditions we'll need information on closings and weather conditions. Remember more is better. We want as much data as possible. So let's see what we can find.\n",
    "\n",
    "### Closing Info\n",
    "\n",
    "I wanted at least 5 years of data, and I was at a loss until I noticed that the Suffolk University Twitter account had been around for that long. So I did a little search for [@Suffolk_U tweets containg the word \"closed.\"](https://twitter.com/search?src=typd&q=closed%20from%3ASuffolk_U) From here I constructed a list of closure, and place them into a spreadsheet with the following columns: `date`, `closed`, and `note`. \n",
    "\n",
    "\n",
    "### Weather Info\n",
    "\n",
    "So we need a historic record of Boston's weather. Luckily, there are a lot of places to find this information. I'm going to use the [Dark Sky API](https://darksky.net/dev/account). An API is an interface that allows a computer program to get structured data on-demand. For example, a program visiting `https://api.darksky.net/forecast/[API_KEY]/42.3605,-71.0596,1314849600` will get a [structured reply](https://darksky.net/dev/docs/time-machine) that it can parse for info. \n",
    "\n",
    "The following blocks of code will make use of the DarkSky API to get historical weather data for Boston for school years 2011/2012 through most of 2016/2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your Dark Sky key? f7c5a140a997189b996690b7f4acdc14\n"
     ]
    }
   ],
   "source": [
    "# When you sign up for API acces, you'll get a KEY. \n",
    "# This will prompt you for that key.\n",
    "\n",
    "os.environ['KEY'] = inputFunc('What is your Dark Sky key? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a list of school days (SY), find the weather on those days,\n",
    "# and place them into a dataframe (df)\n",
    "\n",
    "def run_days():\n",
    "    global SY\n",
    "    global df\n",
    "    for d in SY:\n",
    "        #print (d)\n",
    "        url_1 = \"https://api.darksky.net/forecast/\"+os.environ['KEY']+\"/42.3605,-71.0596,\"+str(round(d.timestamp()))\n",
    "        jsonurl = urlopen(url_1)\n",
    "        output = json.loads(jsonurl.read().decode(\"utf-8\"))\n",
    "        date = d\n",
    "        try:\n",
    "            accum = output[\"daily\"][\"data\"][0][\"precipAccumulation\"]\n",
    "        except KeyError:\n",
    "            accum = 0  \n",
    "        temp_min =(output[\"daily\"][\"data\"][0][\"temperatureMin\"])\n",
    "        temp_max = (output[\"daily\"][\"data\"][0][\"temperatureMax\"])\n",
    "        wind = (output[\"daily\"][\"data\"][0][\"windSpeed\"])\n",
    "        df = df.append({'date':date,'accum':accum,'temp_min':temp_min,'temp_max':temp_max,\"wind\":wind}, ignore_index=True)\n",
    "\n",
    "# Create empty dataframe (df)        \n",
    "df = pd.DataFrame([], columns=[\"date\",\"accum\",\"temp_min\",\"temp_max\",\"wind\"])\n",
    "# Get a set of rules defining bussiness/school days\n",
    "us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "\n",
    "# NOTE: I didn't do a very careful job of defining the school years \n",
    "# because I didn't have copies of the academic calendars for most \n",
    "# years at hand. I really should exclude spring break and the like.\n",
    "# Right now, I'm hoping that I can skip this as there probably aren't\n",
    "# many spring break snow days. ;)\n",
    "\n",
    "# Get weather for Winter 2011 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2011-09-01',end='2011-12-20', freq=us_bd)\n",
    "run_days()\n",
    "# Get weather for Spring 2012 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2012-01-15',end='2012-05-20', freq=us_bd)\n",
    "run_days()\n",
    "\n",
    "# Get weather for Winter 2012 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2012-09-01',end='2012-12-20', freq=us_bd)\n",
    "run_days()\n",
    "# Get weather for Spring 2013 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2013-01-15',end='2013-05-20', freq=us_bd)\n",
    "run_days()\n",
    "\n",
    "# Get weather for Winter 2013 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2013-09-01',end='2013-12-20', freq=us_bd)\n",
    "run_days()\n",
    "# Get weather for Spring 2014 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2014-01-15',end='2014-05-20', freq=us_bd)\n",
    "run_days()\n",
    "\n",
    "# Get weather for Winter 2014 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2014-09-01',end='2014-12-20', freq=us_bd)\n",
    "run_days()\n",
    "# Get weather for Spring 2015 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2015-01-15',end='2015-05-20', freq=us_bd)\n",
    "run_days()\n",
    "\n",
    "# Get weather for Winter 2015 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2015-09-01',end='2015-12-20', freq=us_bd)\n",
    "run_days()\n",
    "# Get weather for Spring 2016 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2016-01-15',end='2016-05-20', freq=us_bd)\n",
    "run_days()\n",
    "\n",
    "# Get weather for Winter 2016 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2016-09-01',end='2016-12-20', freq=us_bd)\n",
    "run_days()\n",
    "# Get weather for Spring 2017 and add to df.\n",
    "SY = pd.DatetimeIndex(start='2017-01-15',end='2017-04-07', freq=us_bd)\n",
    "run_days()\n",
    "\n",
    "df.to_csv('weatherdata.csv',index=False) # Write results to file.\n",
    "df.head() # Give us a peek at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Here we load the data we collected above and get it all ready to feed to our statistical model. This is particuarly useful as I'm not going to give you my API key. So you don't have to get the weather data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and peek at weather data\n",
    "weather_df = pd.read_csv('weatherdata.csv', parse_dates=[0])\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and peek at closing data\n",
    "snow_df = pd.read_csv('snowclosings.csv', parse_dates=[0])\n",
    "snow_df = snow_df.fillna('')\n",
    "#snow_df = snow_df[(snow_df[\"note\"]==\"\") | \n",
    "#                  (snow_df[\"note\"]==\"email\") | \n",
    "#                  (snow_df[\"note\"]==\"facebook\")] # exclude entires with notes\n",
    "snow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge weather and closing data into a single table\n",
    "joint_df = weather_df.merge(snow_df, on='date', how='left')\n",
    "joint_df[\"note\"] = joint_df[\"note\"].fillna('')\n",
    "joint_df[\"closed\"] = joint_df[\"closed\"].fillna(0)\n",
    "joint_df[\"past\"] = joint_df[\"closed\"].rolling(60).sum()\n",
    "joint_df[\"past\"] = joint_df[\"past\"].fillna(0)\n",
    "joint_df.to_csv('jointdata.csv',index=False) # Write results to file.\n",
    "joint_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df = pd.read_csv('jointdata.csv', parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting the Dots (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I found this notebook useful in putting together this demo.\n",
    "# http://nbviewer.jupyter.org/gist/justmarkham/6d5c061ca5aee67c4316471f8c2ae976\n",
    "\n",
    "# Logistic Regression\n",
    "y, X = dmatrices('closed ~ accum',joint_df, return_type=\"dataframe\")\n",
    "y = np.ravel(y)\n",
    "model = LogisticRegression(fit_intercept = False, C = 1e9)\n",
    "mdl = model.fit(X, y)\n",
    "\n",
    "#print(X.head())\n",
    "#print(y)\n",
    "\n",
    "# I found this post useful in putting together the plot\n",
    "# http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html\n",
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.clf()\n",
    "plt.scatter(X[\"accum\"], y, color='black', zorder=20)\n",
    "X_test = np.linspace(-1, 16, 300) # left, right, points\n",
    "\n",
    "def mod(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "loss = mod(X_test * mdl.coef_[0][1] + mdl.coef_[0][0])\n",
    "plt.plot(X_test, loss, color='red', linewidth=3)\n",
    "\n",
    "plt.xlabel('Accumulation in inches')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Suffolk Snow Day?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage of the time was school closed?\n",
    "print(y.mean())\n",
    "# If I always said that there wouldn't be a snow day, how accurate would I be?\n",
    "print(1-y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How accurate is this model?\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using cross-validation\n",
    "# for more on cross-validation check out http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=5)\n",
    "print (\"Accuracy Scores (cv): %s\"%scores)\n",
    "print (\"Average Accuracy (cv): %s\"%scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt user for input and spit out a perdiction.\n",
    "\n",
    "def closed():\n",
    "    os.environ['amount'] = inputFunc('\\nAccumulation of snow (inches)? ')\n",
    "    if os.environ['amount'] != 'e':\n",
    "        prob = model.predict_proba(np.array([[1,float(os.environ['amount'])]]))[0][1] # 1 and accum\n",
    "        print(\"Probability of closure: %.2f %%\"%(prob*100))\n",
    "        closed()\n",
    "        \n",
    "print(\"Type `e` to exit, or interupt the kernel.\")\n",
    "closed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can We Do Better? (Considering Other Factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X_2 = dmatrices('closed ~ accum + temp_min + wind + past',joint_df, return_type=\"dataframe\")\n",
    "y = np.ravel(y)\n",
    "model_2 = LogisticRegression(fit_intercept = False, C = 1e9)\n",
    "mdl_2 = model_2.fit(X_2, y)\n",
    "\n",
    "#print(X_2.head())\n",
    "\n",
    "# check the accuracy\n",
    "print (\"First Model's Accuracy: %s\"%model.score(X, y))\n",
    "print (\"This Model's Accuracy: %s\"%model_2.score(X_2, y))\n",
    "\n",
    "# evaluate the model using cross-validation\n",
    "scores_2 = cross_val_score(LogisticRegression(), X_2, y, scoring='accuracy', cv=5)\n",
    "print (\"This Model's Accuracy Scores (cv): %s\"%scores_2)\n",
    "print (\"This Model's Average Accuracy (cv): %s\"%scores_2.mean())\n",
    "\n",
    "def closed_2():\n",
    "    os.environ['amount'] = inputFunc('\\nAccumulation? ')\n",
    "    if os.environ['amount'] != 'e':\n",
    "        os.environ['low'] = inputFunc('Low temp? ')\n",
    "        if os.environ['low'] != 'e':\n",
    "            os.environ['wind'] = inputFunc('Wind speed? ')\n",
    "            if os.environ['wind'] != 'e':\n",
    "                os.environ['past'] = inputFunc('Closings in last 60 days? ')\n",
    "                if os.environ['past'] != 'e':\n",
    "                    prob = model_2.predict_proba(np.array([[1,float(os.environ['amount']),float(os.environ['low']),float(os.environ['wind']),float(os.environ['past'])]]))[0][1] # 1 and accum\n",
    "                    print(\"Probability of closure: %.2f %%\"%(prob*100))\n",
    "                    closed_2()\n",
    "        \n",
    "print(\"\\nType `e` to exit, or interupt the kernel.\")\n",
    "closed_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
