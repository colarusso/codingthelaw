{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing\n",
    "\n",
    "So here's the deal, you could take this notebook, read through it, run it, and turn in its output, and that would meet expecations for this week. Of course, my hope is that you'll do more than this, but it's important for you to know that you don't have to unless you want to exceed expecations. So I'm attaching a stretch goal to incentie a little more than meets expectations, create a model that gets an F1 score in excess of 0.7 on the challenge data. The point is, if this dosn't seem doable for you, I don't want you spending too much time on this. \n",
    "\n",
    "\n",
    "# Load Libraries\n",
    "\n",
    "You don't need to worry about reading through the code in this first cell. It's just us loading libraries and functions. In the training notebook, these were spread out, but here we've moved them all up to the front. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# I'll need these libraries to make it work\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "    \n",
    "def evaluate(prob, pred, labels_test, verbose=1):\n",
    "    acc = accuracy_score(labels_test,pred)\n",
    "    ones = sum(labels_test)/len(labels_test)\n",
    "    zeros = (1 - ones)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test, pred).ravel()\n",
    "\n",
    "    recall_ = tp / (tp + fn) \n",
    "    precision_ = tp / (tp + fp)\n",
    "    f1 = (2 / ((1/recall_)+(1/precision_)))\n",
    "\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(labels_test,prob)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    if verbose==1:\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = [14, 4]\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "        label='AUC = %0.2f'% roc_auc)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.plot([0,1],[0,1],'r--')\n",
    "        plt.xlim([-0.1,1.2])\n",
    "        plt.ylim([-0.1,1.2])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(labels_test,prob)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.plot(recall, precision,'b',\n",
    "        label='AP = %0.2f'% precision.mean())\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlim([-0.1,1.2])\n",
    "        plt.ylim([-0.1,1.2])\n",
    "        plt.ylabel('Recall')\n",
    "        plt.xlabel('Precision')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.hist([labels_test,prob], label=[\"true\",\"pred\"], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.title(\"Predictions vs Reality\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlim([-0.1,1.2])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()    \n",
    "\n",
    "        print (\"\\n0s: %0.2f\\tTrue Positives: %s\\tAccuracy: %s \"%(zeros,tp,acc))\n",
    "        print (\"1s: %0.2f\\tTrue Negatives: %s\\tAUC: %s\"%(ones,tn,roc_auc))\n",
    "        print (\"\\t\\tFalse Positives: %s\\tF1 Score: %s\"%(fp,f1))\n",
    "        print (\"\\t\\tFalse Negatives: %s\\tRecall (fract of actual yeas found): %s\"%(fn,recall_))\n",
    "        print (\"\\t\\t\\t\\t\\tPrecision (correctness of yeas predicted): %s\\n\"%precision_)        \n",
    "        \n",
    "        null = zeros\n",
    "        if null < ones:\n",
    "            null = ones\n",
    "        \n",
    "        if (acc>null) and (roc_auc>0.5) and (recall_>0.5) and (precision_>0.5):\n",
    "            print(\"################\")\n",
    "            print(\"#     PASS     #\")\n",
    "            print(\"################\")\n",
    "            vpass = \"Yes\"\n",
    "        else:\n",
    "            print(\"################\")\n",
    "            print(\"#     FAIL     #\")\n",
    "            print(\"################\")\n",
    "            vpass = \"No\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load your labeled data (people.csv and calls.csv)\n",
    "\n",
    "Broadly speaking, what you need to do this week is load the labeled data, clean it, train a model, load the challenge data, clean it, then run it through the modle you trained before. The labeled and challenge data have the same format, except the challenge data doesn't have a _take_ column. That means, you should be performing the same cleaning steps for both sets of data. Earlier in discussion with folks, I suggested that folks just copy their cleaning steps, but to make it easier on this notebook, I'm just going to ask you to come back up to this part of the notebook and run a differnt set of data through the steps. \n",
    "\n",
    "Anywho, the way we'll make this work is by running through this notebook multiple times. The first time through you'll load the labeled data, clean it and tran your models. The second time through you'll load the challenge data, pass it through the model you trained and collect it's output. \n",
    "\n",
    "This next cell promptes you to say if you're doing the first run through (training) or the second (prediction). Based on your answer it will either load the labled data or the challenge data. If you're doing the former, it will also ask you if you want to limit the size of your training data to make things run faster. \n",
    "\n",
    "After loading your data, this cell will deal with datatypes and make sure that the dataframes use a common column name for the person_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pass_through = input(\"Training or Predicting? (t or p): \") \n",
    "\n",
    "if pass_through == \"t\":\n",
    "    df_1 = pd.read_csv('people.csv', parse_dates=[4]) \n",
    "    df_2 = pd.read_csv('calls.csv', parse_dates=[11,12])\n",
    "    print(\"\\nIf you'd like to limit the number of rows loaded, provide a row limit. Otherwise leave the promt blank.\")\n",
    "    limit_rows = input(\"Row limit?\")        \n",
    "    if limit_rows != '':\n",
    "        limit_rows = int(limit_rows)\n",
    "else:\n",
    "    df_1 = pd.read_csv('challenge_people.csv', parse_dates=[4])    \n",
    "    df_2 = pd.read_csv('challenge_calls.csv', parse_dates=[11,12])\n",
    "\n",
    "print()\n",
    "    \n",
    "df_1[\"date of birth\"] = pd.to_datetime(df_1[\"date of birth\"], errors='coerce')\n",
    "print(df_1.dtypes)\n",
    "display(df_1.head())\n",
    "\n",
    "df_2 = df_2.rename(columns={'person_ID': 'person_id'})\n",
    "print(df_2.dtypes)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Your Data, Clean Your Data, and Build Features\n",
    "\n",
    "Here we'll combine our dataframes together into a single dataframe we will call `single_table`.\n",
    "\n",
    "Note: If you want to exceed expectations, this is where you would do it by adding or subtracting features. I've created a few features, but the performance of this model isn't very good. To pass an F1 of 0.7, you'll need to think more about what features really need to be here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table = df_2.merge(df_1, how=\"left\", on=\"person_id\") \n",
    "single_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to get a feel for the column names, let's print them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the _Unambed: 0_ column was duplicated. So will need to get rid of the `_x` and `_y` columns later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create features based on dates (i.e., the would-be client's age and the time since their incident). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table[\"age\"] = (pd.to_datetime(single_table[\"intake\"]) - pd.to_datetime(single_table[\"date of birth\"])).astype('timedelta64[Y]')\n",
    "single_table[\"time since\"] = (pd.to_datetime(single_table[\"intake\"]) - pd.to_datetime(single_table[\"injury_date\"])).astype('timedelta64[Y]')\n",
    "single_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we confront the issues mixed case by makeing all the columns with strings uppercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table['Referal Soure'] = single_table['Referal Soure'].str.upper()\n",
    "single_table['body_part_1'] = single_table['body_part_1'].str.upper()\n",
    "single_table['body_part_2'] = single_table['body_part_2'].str.upper()\n",
    "single_table['body_part_3'] = single_table['body_part_3'].str.upper()\n",
    "single_table['body_part_4'] = single_table['body_part_4'].str.upper()\n",
    "single_table['body_part_5'] = single_table['body_part_5'].str.upper()\n",
    "single_table['surgery'] = single_table['surgery'].str.upper()\n",
    "single_table['sex'] = single_table['sex'].str.upper()\n",
    "\n",
    "# If this is the training pass, we'll want to make sure the take column is uppercase as well. \n",
    "if 'take' in single_table.columns:\n",
    "    single_table['take'] = single_table['take'].str.upper()\n",
    "\n",
    "single_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We want to turn our surgey column into mnumbers. First we need to make sure we understand what's in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table['surgery'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, just NO and YES values. So let's turn the NOs to 0 and the YESes to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table.loc[single_table['surgery'] == \"YES\", 'surgery'] = 1\n",
    "single_table.loc[single_table['surgery'] == \"NO\", 'surgery'] = 0\n",
    "single_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should do the same thing with the take column, assuming we're on the training pass where single_table has a take column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'take' in single_table.columns:\n",
    "    \n",
    "    print(single_table[\"take\"].unique())\n",
    "    \n",
    "    single_table.loc[single_table['take'] == 'YES', 'take'] = 1\n",
    "    single_table.loc[single_table['take'] == 'Y', 'take'] = 1\n",
    "    single_table.loc[single_table['take'] != 1, 'take'] = 0\n",
    "    \n",
    "    display(single_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I showed you this code in class, but here it is for you to use. What's going on here is that we're creating dummy variables for each body part column and then grouping them all together. **This could take a few minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note this cell may take a while to run\n",
    "\n",
    "parts = pd.get_dummies(single_table['body_part_1'])\n",
    "parts = pd.concat([parts, pd.get_dummies(single_table['body_part_2'])], axis=1)\n",
    "parts = pd.concat([parts, pd.get_dummies(single_table['body_part_3'])], axis=1)\n",
    "parts = pd.concat([parts, pd.get_dummies(single_table['body_part_4'])], axis=1)\n",
    "parts = pd.concat([parts, pd.get_dummies(single_table['body_part_5'])], axis=1)\n",
    "parts = parts.groupby(by=parts.columns, axis=1).any().astype(int)\n",
    "parts.head()\n",
    "\n",
    "# note this cell may take a while to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take this set of new columns and add them to the existing table by using concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table = pd.concat([single_table, parts], axis=1)\n",
    "single_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing with other variables for which we need dummy variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_table = pd.concat([single_table, pd.get_dummies(single_table['Referal Soure'])], axis=1)\n",
    "single_table = pd.concat([single_table, pd.get_dummies(single_table['sex'])], axis=1)\n",
    "single_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've gotten to the spot where we drop the columns we don't want to consider. You'll note that I didn't remove the call_id. That was on purpose. After this cell, we'll make sure everything is numbers and get rid of those rows that aren't numbers. At that time, we'll pull out the call_ids, but only then because if you're in your second run through (prediction) we'll need to match those IDs up with your predictions and we need to make sure that we aren't changing the row order after we pull out that list. That is, we make sure that the row order is fixe before we caputure a list of the IDs.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_num = single_table.drop([\n",
    "                                            'Unnamed: 0_x',\n",
    "                                            'Unnamed: 0_y',\n",
    "                                            'name',\n",
    "                                            'Referal Soure'\n",
    "                                            ,'body_part_1'\n",
    "                                            ,'body_part_2'\n",
    "                                            ,'body_part_3'\n",
    "                                            ,'body_part_4'\n",
    "                                            ,'body_part_5'\n",
    "                                            ,'sex'\n",
    "                                            ,'attorney'\n",
    "                                            ,'injury_date'\n",
    "                                            ,'intake'\n",
    "                                            ,'date of birth'\n",
    "                                            ,'person_id'\n",
    "                                           ], 1).copy()\n",
    "print(\"row count:\",len(df_num),\"\\n\")\n",
    "\n",
    "print(df_num.dtypes)\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ane let's just make sure that everything in this table is a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_num = df_num.apply(pd.to_numeric, errors='coerce')\n",
    "# errors='coerce' will set things that can't be converted to numbers to NaN\n",
    "# so you'll want to drop these (NaNs) like so.\n",
    "print(\"row count before drop:\",len(df_num))\n",
    "df_num = df_num.dropna()\n",
    "print(\"row count after drop:\",len(df_num))\n",
    "display(df_num.head())\n",
    "df_num.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where we pull out the call_ids. First we put them into a list, then we remove the column from df_num."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_ids = df_num[\"call_id\"]\n",
    "\n",
    "df_num = df_num.drop(['call_id'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if pass_through.lower().strip() == \"t\":\n",
    "    d = 1\n",
    "else:\n",
    "    d = 0\n",
    "    print(      \"\\n==============================\\n\")\n",
    "    display(HTML('  You have already trained\\n a model(s). Jump to: <a href=\"#Making-Predictions\">Making Predictions</a> and run its cells.'))\n",
    "    print(       \"\\n==============================\\n\")\n",
    "    \n",
    "1/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create class_df from df_num. If you set a row limit above, limit class_df to the number of rows you set, otherwise do nothing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if limit_rows != '':\n",
    "    class_df = df_num.sample(n=limit_rows)\n",
    "else:\n",
    "    class_df = df_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break your dataframe into training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe containing a random sample of rows\n",
    "class_holdout = class_df.sample(frac=0.90)\n",
    "\n",
    "# create a dataframe that conatins the rows from except for those in holdout\n",
    "class_training = class_df.loc[~class_df.index.isin(class_holdout.index)]\n",
    "\n",
    "# make a training dataframe containing just features\n",
    "features_train = class_training.drop(\"take\", axis=1).values\n",
    "\n",
    "# make a training dataframe containing only the target\n",
    "labels_train = class_training[\"take\"].values\n",
    "\n",
    "# make a testing/holdout dataframe containing just features\n",
    "features_test = class_holdout.drop(\"take\", axis=1).values\n",
    "\n",
    "# make a testing/holdout dataframe containing only the target\n",
    "labels_test = class_holdout[\"take\"].values\n",
    "\n",
    "print(\"size of training\",len(class_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept = False, C = 1e9)\n",
    "clf_1 = model.fit(features_train, labels_train)\n",
    "pred = clf_1.predict(features_test)\n",
    "prob = clf_1.predict_proba(features_test)[:,1] \n",
    "print(\"Logistic Regression\")\n",
    "evaluate(prob, pred, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_3 = RandomForestClassifier()\n",
    "clf_3 = clf_3.fit(features_train, labels_train)\n",
    "pred = clf_3.predict(features_test)\n",
    "prob = clf_3.predict_proba(features_test)[:,1] \n",
    "print(\"Random Forest\")\n",
    "evaluate(prob,pred, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if pass_through.lower().strip() == \"t\":\n",
    "    d = 0\n",
    "    print(\"\\n==============================\\n\")\n",
    "    display(HTML('   Before you make predictions\\n Go back and rerun the cells above\\n from <a href=\"#Load-your-labeled-data-(people.csv-and-calls.csv)\">Load-your-labeled-data</a> through Training'))\n",
    "    print(\"\\n==============================\\n\")\n",
    "else:\n",
    "    d = 1\n",
    "    \n",
    "1/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set clf to your best model above, and set input equal to df_num. Keep in mind that since you only run this cell during your second passthrough (prediction), df_num is now the challenge data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What classifier do you want to use? Above, each one is \n",
    "# given it's own number (e.g., clf_1, clf_2, etc.)\n",
    "clf = clf_3\n",
    "\n",
    "# Set the values for each of the above features: accum, min, max, wind\n",
    "inputs = df_num.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe with your predictions and the call IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_list = pd.DataFrame(\n",
    "    {\n",
    "        'call_id': call_ids, \n",
    "        'take': clf.predict(inputs)\n",
    "    })\n",
    "prediction_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter this new dataframe such that it only includes those with a predicted take and show us the call_ids for those rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_list[prediction_list[\"take\"]==1][\"call_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
