{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preamble\n\n### Are You in the Right Place?\n\nThe following is part of a multi-part introduction to data science for those in the legal profession. The full collection of materials can be found on the Suffolk LIT Lab's How To page under the heading [Demystify Data Science](http://suffolklitlab.org/howto/#demystified). If you've followed the instructions found at [Data Warngling and Feature Engineering](http://suffolklitlab.org/howto/demystified/1/), you should have this notebook running as opposed to viewing a preview. That means that you can run the code fond below, not to mention, read and write files to your own copy of this library.\n\n### A Quick Test/How To\nTo run the code in a given cell (one of the gray boxes), make sure that it has focus (i.e., is highlighed by a bounding box), then click the \"Run\" button in the menu above. Alternativly, you can press `Shift+Enter`. To give a cell focus, just click on the cell. Lets give it a try. **Run the cell below.**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Yay! It worked.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If the text \"Yay! It worked.\" appreared after the cell, it worked. Yay! FYI, you are welcome to join the LIT Lab's Slack Team. There you can ask and answer questions relating to this lesson under the [#howto](https://app.slack.com/client/T8UJQGP47/CLQFA7T7G) channel. See the Lab's [How To](file:///H:/LITLab/SuffolkLITLab.github.io/howto/) for more. That being said, let's get to the main course. \n\nAs you come upon cells, run them. FYI, the text blocks are actual cells too. So it's perfectly reasonable to press `Shift+Enter` to move your way down the page. If you want to see how we format text, double click on one of the text blocks and you'll see something called [markdown](https://en.wikipedia.org/wiki/Markdown). You can set a cell to \"Code\" or \"Markdown\" in a pulldown menu above. We're not going to do anything with Markdown here, but I thought you'd like to know. Anywho, to convert the Markdown back to text, just run the cell.  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data Wrangling and Feature Engineering\n\n## A Rose by any Other Name (Terms of Art)\n\nJust like law, data science makes use of words in a peculiar way. Often the vernacular use gives you a sense of what is meant, but behind that is a set of shared assumptions. So let's define some terms and explore a few concepts particular to the Python programing language. Most of these concepts are generalizable to other languages, but since we'll be using Python, we'll stick with that. \n\n### Variables (names for your data)\n\nVariables are containers for data. To define a variable you type the variable name, an equal sign, and then the content, like so:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "variable_example = \"content goes here\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Running that cell didn't produce any out put. Did something really happen? Well, let's see. Including a variable at the end of a code block (e.g., one of these cells) will cause it to spit out its contents for viewing when run, like so:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "variable_example",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you want to see the contents of multiple variables in a code block when it is run, you will need to use the print function. Here are some examples"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "variable_example_1 = \"content\"\nprint(variable_example_1)\n\nvariable_example_2 = \"more content\"\nprint(variable_example_2)\n\nprint(variable_example_1,variable_example_2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you want to mix in some text for context and make things easier to read, you can do so like this:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Variable One:\",variable_example_1,\"\\nVaraible Two:\",variable_example_2) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note: `\\n` is code for *new line*. There are actually a lot of ways to mix text and variables, but this will do for now.\n\nA quick word about *functions.* A function is like a little program that you can pass information to. In the case of the print function, you pass it some data and it spits it back out to the screen/notebook. We'll see more functions later on. So keep this in mind. \n\n### Datatypes (containers for your data)\n\nThere are different types of containers (datatypes). For our purposes, you need to know about: \n\n**(1) Numbers.** Created by typing a number with no quotes. There are actually several types of number-based datatypes, but we'll save that discussion for later."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "number_example = 1 \nprint(\"number_example =\",number_example)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**(2) Strings.** Created by typing a string of text (which can include numbers, special characters, and the like) inside quotes. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "string_example = 'this worked too'\nanother_string_example = \"This is were the content goes! Ain't that cool?\" \nprint(\"string 1:\",string_example,\"\\nstring 2:\",another_string_example)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note: by using double quotes in the second string I didn't have to worry about the single quote/apostrophe closing the container. Another option to avoid this problem (when a string contains the character used to define the string's boundaries) is to use an escape character. In python that character is a `\\`. Placing it in front of the closing character will cause it to be treated as that character, not the end of the string. We've seen `\\` before as part of a special \"code\" for a *new line*. For more on this, like how to insert a tab, see https://www.codecademy.com/en/forum_questions/52f31477282ae3c473002317 \n\nLet's see some escape characters in action. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "messy_string = \"This string contains a contraction and a quote, \\\"Ain't that cool?\\\"\"\nprint(messy_string)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We're not going to use this here, but you can also use triple quotes to create multi-line strings."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "big_string = \"\"\"  \n                This is a mult-line string with a contraction and quote. \n                \"Ain't that cool?\"\n\"\"\"\n\nprint(big_string)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**(3) Lists.** Created by placing strings or numbers inside brackets and seperated by commas."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "list_example = [\"text\", 2, \"more text\"]\nprint(list_example)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**(4) Dataframes.** Unlike the previous datatypes, dataframes don't work in Python out of the box. To use a dataframe you have to use a library like [Pandas](https://pandas.pydata.org/). What is a dataframe? Basically it's a table of data, like a spreadsheet. You can build a dataframe from scratch by defining the cloumns and adding rows, but most of the time you'll probably create a dataframe from some existing tabular data, like a csv file. Before we can do any of that though, well need to import the Pandas library. You can only do this if the library is installed on your computer. If it isn't, you'll get an error. For more on installing a library, check out these [instructions](https://github.com/colarusso/measured_justice/blob/master/README.md#download-install-and-run-notebooks). If you're running this on Azure Notebooks, Pandas should already be installed. Anyhow, here's how you load the library. You only have to do this once after opening a notebook. So customarily, I place all my load calls at the top of my notebooks. However, today we'll take the approach that we'll load things as we need them."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The above code is telling your notebook to load the `pandas` library as `pd`. That means that instead of typing out `pandas` when referencing the library we can just type `pd`.\n\nOkay, now we're ready to create a dataframe from scratch. Note: instead or `print()`, use `display()` to show the contents of a dataframe."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.DataFrame([[1,\"Thing 1\"]], columns=[\"id\",\"thing\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create an empty dataframe\ndf_example = pd.DataFrame([],columns=[\"id\",\"thing\"]) \n\n# append a new row\ndf_example = df_example.append(pd.DataFrame([[1,\"Thing 1\"]], columns=[\"id\",\"thing\"]), ignore_index=True)\n\ndisplay(df_example)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did you catch what I did there with comments? That's right, you can comment out text in your code by placing a `#` in front of it. Commenting something out just means that when the code runs it will be ignored. If you didn't do this, you'd get all sorts of errors as \"create an empty data frame\" isn't a valid python command. Likewise, you can comment out code so that it won't run when the cell runs. This can be useful if you want to keep something around to remind you of another way you might have done something. \n\n**So why do we have data types?** Because you can do different things with different datatypes. The most obvious is math. You can't do math on strings. Consider the following two examples: "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = 1\ny = 2\nz = x + y\nprint(x,\"+\",y,\"=\",z)\n\n# versus\n\nx = \"1\"\ny = \"2\"\nz = x + y\nprint(x,\"+\",y,\"=\",z)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That second expression isn't saying that 1 + 2 = 12. It's taking the string \"1\" and \"adding\" the string \"2\" to the end.\n\n**So how do I do math?** For the most part you make sure you're dealing with a number datatype and just type things out like you might on a calculator. You have to make sure to use the right characters/operators however. For a list of these, check out this [article on Operators](https://www.programiz.com/python-programming/operators)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load and Peek at Your Data\n\nAs mentioned above, you're probably going to be loading data into dataframes from a file. So let's give that a try. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load data into a variable and peek at it. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n# here you'll notice that pandas (abbreviated pd) has a way to pull data\n# directly from a csv and place it in a dataframe\n\ndf_1 = pd.read_csv('table_1.csv') \n\ndf_1.head() # I could have encased the df in display(), but since it is the last\n            # thing in the cell, I can count on it to spit out its content.\n    \n# This table contains the dates of school closings at Suffolk University, \n# along with some notes.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Like Lists, dataframes can be made up of different datatypes. We can use `.dtypes` to see what they are in a particular instance. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_1.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`Object` is Pandas speak for `string.` See http://www.datacarpentry.org/python-ecology-lesson/03-data-types-and-format/ That sounds good, but I bet you might want to treat those dates as something other than strings. That is, you may want to compare them and add to then, you know math-like things. Well it so happens that there are date datatypes, and we can tell Pandas to treat specific columns as dates when loading like so:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# load file into dataframe\ndf_1 = pd.read_csv('table_1.csv', parse_dates=[0]) \n\n# An important thing to note is that python tends to start counting from `0`. \n# So what the above is saying is that it should treat the first column as a date.\n\n# Display the first bit of the dataframe\ndisplay(df_1.head())\n\n# List the datatypes for each column\ndf_1.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can see that the first column is now the data type `datetime64[ns]`. This isn't a data type native to Python, but there are ways to have it play nice with dates in Python. That being said, we'll leave that for later.\n\nWhile we're here, what's with all the `NaN` entries under the `note` column? Well if you were to open up the csv file, you'd find that they correspond to nothing, literally nothing, there's no data there. Pandas displays such null (computer speak for empty) values as Not a Number or `NaN`."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# I have two data files. So I'll repeat the above but place the second file in \n# A second file.\n\ndf_2 = pd.read_csv('table_2.csv', parse_dates=[0]) \ndf_2.head()\n\n# This table contains several years of weather information for Suffolk University\n# The time involved is the same as that of the first dataframe, but there are \n# missing days.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Wait something's off. The date column isn't in the right format (YYYY-MM-DD). Let's check the data types."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_2.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you were to go through and look at each of the entries, you'd notice that some of them are of the wrong format (e.g., 1/8/200156). These are clearly typos, and we need to fix or get rid of them. \n\nThere are functions for converting the contents of an entire column into another data type. So you can change numbers to strings etc. Here we'll try to convert the date column into dates like so: **be prepared for some colorful output as there's going to be an error.**"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# the below says set the column dates equal to the column dates after converting everything to the proper data type\n\ndf_2[\"date\"] = pd.to_datetime(df_2[\"date\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Well that gave us a big old error. The bit following \"VuleError:\" at the end gives us a clue as to what's going wrong (i.e., `ValueError: year 20125 is out of range`).\n\nThis means it found a year 20125, and it knows that's not a year, at least not for a long time. If we follow the error output back to the top we can see where it all went wrong, but that's for another time.\n\nIf we aren't going to fix such an error in the original source, we can tell the function to ignore them, Here we'll ask the function to turn these \"bad\" dates into blank entires similar to NaNs (note `errors='coerce'`). See errors under https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_2[\"date\"] = pd.to_datetime(df_2[\"date\"], errors='coerce')\ndisplay(df_2.head())\ndf_2.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That's better, now let's figure out how many rows we're dealing with. \n\nThe function `len()` measures how long something is. It doesn't work for all datatypes, but for a dataframe it counts the number of rows. So let's take a look."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"table 1 row count:\",len(df_1))\nprint(\"table 2 row count:\",len(df_2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Questions?\n\nEventually, we're looking to use this data to answer some question. To do that, we'll need two types of columns: (1) features; and (2) a single column containing the target. The target is what we're trying to predict, and the features are those things that might help predict the target. The data we're working with was collected with the idea that we could predict whether or not Suffolk University would call a snow day. In that senerio, `closed` is our target and everything else is a potential feature.\n\n## Recipes for Cleaning (and exploring) Your Data\n\nAt the end of the day, we're probably going to want all of our data in one table/dataframe. And most likely we're going to want that dataframe to contain only numbers. Why? Because we're focusing on training models, and those models want nice big tables containing only numbers because those models are statistical in nature. They work by doing math, and remember, you can't do math with strings. In truth, it's possible that our model might be okay with a target column that didn't contain numbers as long as that column contained a set of labels (e.g., snow day, no snow day). However, labels can easily be represented by numbers (e.g., 1 = snow day, 0 = no snow day). FYI, it's very common to have 1 mean yes and 2 mean no. \n\nAnyhow, the process of cleaning ones data is often called data wrangling or data munging. It's not very exciting, but it's very consequential. I once wrote a brief for the Massachusetts Supreme Judicial Court which was almost entirely a description of data wrangling. So let's get started. \n\nWhat follows is a set of recipes you can come back to in the future as you need to clean data. They also serve as survey of what you can do to clean data in Python. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# You can merge dataframes like so. See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\n# See how we're referencing the variable names we gave the dataframes earlier.\nsingle_table = df_2.merge(df_1) \nprint (\"row count:\",len(single_table))\nsingle_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Wait. That's not right! Why are there only 19 rows? The answer? It has to do with how we joined the tables. There are several ways to join tables, and you should become familar with them. See e.g.: ![joins](https://i.stack.imgur.com/udQpD.jpg) \n\nWe want a left join on the table of weather data, not the closed dates, because the weather data is the larger dataset. By the way, we can also define the colums we want to join. By default this is just columns with the same name, but sometimes you don't always want to join on all such columns. For fun, I just made things explicit in the code (i.e., `on='date'`)."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "single_table = df_2.merge(df_1, on='date', how='left') \nprint (\"row count:\",len(single_table))\nsingle_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That's better. Now we can look at a single column like this:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "single_table[\"accum\"].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Fun fact, you can run functions on the contents of a column. For example:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# find the mean of the entires:\nprint(single_table[\"accum\"].mean())\n\n# find the biggest entry:\nprint(single_table[\"accum\"].max())\n\n# find the smallest entry:\nprint(single_table[\"accum\"].min())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And it's not just math, you can explore unique entires by stating the column and using `.unique()`. See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(single_table[\"note\"].unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can explore rows with a specific value like so:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "tmp = single_table[single_table[\"closed\"]=='yes']\nprint(\"row count:\",len(tmp))\ntmp.head() \n\n# Note: here we're taking a slice of the dataframe/table and putting it\n# in a new dataframe called tmp. You could use this approach to get\n# a dataframe where some thing is true (e.g., all the \"closed\" entires)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The above slice, however, would miss any yeses with different capitalization (e.g., Yes or YES). To avoid this issue, you can just force everything in a column to be uppercase, like this"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "single_table['closed'] = single_table['closed'].str.upper()\ntmp = single_table[single_table[\"closed\"]=='YES']\nprint(\"row count:\",len(tmp))\ntmp.head() \n\n# Note, you can make something lowercase by using .lower()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can invert a match by using the 'not equal' evaluation (i.e., `!=`)."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "tmp = single_table[single_table[\"closed\"]!='YES']\nprint(\"row count:\",len(tmp))\ntmp.head() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Evaluations work with numbers too. Consider the greater than comparison. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "tmp = single_table[single_table[\"accum\"]>1]\nprint(\"row count:\",len(tmp))\ntmp.head() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For a complete list of ways to compare strings and numbers, check out the Comparison Operators sections from the [Operators post](https://www.programiz.com/python-programming/operators) from above. After that, if you didn't already, read up on what other operators do (e.g., +, -, \\*).\n\nFor the special case of when a value is NaN, you can filter based on the value not being null (i.e., empty) this involves using a pandas function `pd.notnull()` that checks the contents and returns `True` if it's not null."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "tmp = single_table[pd.notnull(single_table[\"closed\"])]\nprint(\"row count:\",len(tmp))\ntmp.head() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The invers of `notnull` is `isnull`."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "tmp = single_table[pd.isnull(single_table[\"closed\"])]\nprint(\"row count:\",len(tmp))\ntmp.head() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can  make a copy of a dataframe like so. This will allow you to work on a copy of the data without changing the original. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table = single_table.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you have a column that contains a set of labels, you can turn them into multiple rows containing numbers like so: "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table = pd.concat([processed_table, pd.get_dummies(processed_table['note'])], axis=1)\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()\n\n# The value for a column will be 1 if the source column contained that label. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can remove unwanted colums like so:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# for a single column\nprocessed_table = processed_table.drop('email', 1)\nprint(\"row count:\",len(processed_table))\ndisplay(processed_table.head())\n\n# for multiple columns\nprocessed_table = processed_table.drop([\n                                            'early',\n                                            'early facebook',\n                                            #'facebook'\n                                           ], 1)\nprint(\"row count:\",len(processed_table))\ndisplay(processed_table.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Alternatively, if you want to make a new table from a subset of columns, you can do so like we do below. By using the dataframe `single_table` below I am pulling form the original data not the copy. However, I am careful to place `.copy()` at the end to make sure I'm not changing the original dataframe."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table = single_table[[\n                                        'date',\n                                        'accum', \n                                        'temp_min',\n                                        'temp_max',\n                                        'wind',\n                                        'closed'\n                                     ]].copy()\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can rename columns like so."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table = processed_table.rename(columns={\n                                                        'temp_min': 'min', \n                                                        'temp_max': 'max'\n                                                     })\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can replace values in a column based on logic like so:"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table.loc[processed_table['closed'] == 'YES', 'closed'] = 1\nprocessed_table.loc[processed_table['closed'] != 1, 'closed'] = 0\n\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Feature Engineering\n\nFeature engineering is the process of creating new columns from your old ones. This is done to make the data more usable. \"How?\" you ask. Well let's say you thought a useful feature might be weather or not it's freezing. We don't have a column for that. We have the information needed, but it's not a feature. To fix this we can add a column. I'll show you how in a bit. \n\nYou can add a new column like so. Here the new column \"mid\" is equal to the average of the day's high and low temp."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table[\"mid\"] = (processed_table[\"min\"] + processed_table[\"max\"])/2\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you want to make new columns based on some conditional statements, not equations, there are actually a number of ways to do this, but a straight-forward method is as follows. Create a new column where all the cells contain the same value."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table[\"freezing\"] = 0\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()\n\n# Here all instances of the new column contain a zero.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Then customize these new column using the method decribed above for replacing values in a column based on logic."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table.loc[processed_table['min'] <= 32, 'freezing'] = 1\nprocessed_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Heck you can even string logic together. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table[\"cold_and_windy\"] = 0\nprocessed_table.loc[(processed_table['wind'] >= 5) & (processed_table['min'] <= 32), 'cold_and_windy'] = 1\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember dates? Well you can work with those too. A full description of how is beyond the scope of this post, but for more you might want to look at https://docs.python.org/3/library/datetime.html That being said, here's a glimpse."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# first you'll need this library\nfrom datetime import datetime, timedelta\n\nprocessed_table[\"days_passed\"] = (datetime.today() - pd.to_datetime(processed_table[\"date\"])).astype('timedelta64[D]')\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()\n\n# This just makes a new column with number of days between today and the date in the date column. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is a little advanced (that means I'm not expecting you to understand everything that is about to happen), but let's create our own function. To do this, type \"def\" followed by the name you want to give your function, a parenthetical, and a colon. Inside the parenthetical type a variable name that will be used within the function to reference the data passed to it. More advanced functions can have multiple variables."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def yesterdays_high(row):\n    # Here we're thinking of row as one of the rows from our dataframe\n    # So row = the content of a single row\n    \n    # Find yesterday's date by subtracting 1 day from the date column\n    yesterday = pd.to_datetime(row[\"date\"])-timedelta(days=1)\n    \n    # Let the function know that we can access the processed_table dataframe\n    # We do this because the variables defined outside a function aren't \n    # accessible by default. global [variable] lets the function know it can\n    # look outside the function for this variable\n    global processed_table\n    \n    # If yesterday is after or equal to the earliest date in the dataframe, do a thing.\n    if yesterday >= processed_table[\"date\"].min():\n        # Set yesterday's max to that of the previous day.\n        # I'm doing some weird stuff here, don't worry too much about it right now.\n        # but mostly, I'm setting yesterdays_max to equal the high 'yesterday'\n        yesterdays_max = str(processed_table.loc[processed_table['date'] == yesterday][\"max\"].tolist()).strip('[]')\n        \n    # If yesterday is before the earliest date in the dataframe, do a thing.\n    else:\n        # set yesterday's date to \"\"\n        yesterdays_max = \"\"\n\n    # return is a command telling the function to spit out what follows.\n    # so this statement means that when the function is called it will \n    # spit out yesterdays_max\n    return yesterdays_max\n\n# I'm not doing it here, but you can imagine tweaking the above to deal with yesterdays highs and lows. \n# What I'm thinking is something that takes in the row plus a variable directing it to look at highs or lows.\n# The production of such is left as an exercise for the reader. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have this new function, we can apply it to an entire dataframe and use it's output to fill a new column, by default the value of the row is passed to the function."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table[\"yesterdays_high\"] = processed_table.apply(yesterdays_high, axis=1)\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Since we've played around a little, let's trim our dataframe down a bit so it only contains those columns we really want (e.g., just the numbers)."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table = processed_table[[\n                                        'accum', \n                                        'min',\n                                        'max',\n                                        'wind',\n                                        'yesterdays_high',\n                                        'closed'\n                                     ]]\nprint(\"row count:\",len(processed_table))\nprocessed_table.head()\n\n# Note, I didn't use '.copy()' like above because I'm fine with overwriting \n# the original (i.e., processed_table)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Make sure everything really is a number.\n\nOkay, let's see what datatypes we've ended up with."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember how I said there were several types of numbers. Well, now it's worth talking about them. For our purposes, if you see `int64` or `float64` in a dataframe, it's a number. See [Data Types and Formats](http://www.datacarpentry.org/python-ecology-lesson/03-data-types-and-format/) for details. Remember, however, that `object` is pandas for `string`.\n\nHow did this happen? When I look at the dataframe `yesterdays_high` and `closed` look like they're just numbers? Well, think about how we made them. `Closed` started its life containing \"yes\" and blank spaces. As for `yesterdays_high`, the function we used had to convert things into a string at one point. \n\nTo make sure all of your columns are stored as numbers, use the `pd.to_numeric` method like so. This will attempt to change everything into a number. This is the same idea as when we turned the date column into dates, except here we're applying the change to all columns at once, and we're dealing with `to_numeric`, not `to_datetime`."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "processed_table = processed_table.apply(pd.to_numeric, errors='coerce')\n# errors='coerce' will set things that can't be converted to numbers to NaN\n# so you'll want to drop these (NaNs) like so.\nprint(\"row count before drop:\",len(processed_table))\nprocessed_table = processed_table.dropna()\nprint(\"row count after drop:\",len(processed_table))\ndisplay(processed_table.head())\nprocessed_table.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Okay, that seemed to do the trick. We did, however, lose some rows. Mostly these would have been those rows where we couldn't find yesterday's temp. \n\nFYI, if instead of removing `NaN`s you wanted to replace them with a set value (e.g., `0`), you can use `.fillna()`. See https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.fillna.html\n\nNow that we have clean data we're ready to do something with it. Up next, Training Models. However, we don't want to lose all of our work. Let's save our dataframe to a `.csv` file. So we can load it into our next notebook. Here's the line:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 'index_label=False' is just so we don't save the row counts in the first colum. \nprocessed_table.to_csv(\"processed_table_only_numbers.csv\", index_label=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Alright, we're good to go. If you're feeling up to it, you can jump on over to [How to Train a Model](http://suffolklitlab.org/howto/demystified/2/). Note: if you're working through these notebooks as part of a class with [Colarusso](http://suffolklitlab.org/team/#colarusso), this is probably something you'll do together in class. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}