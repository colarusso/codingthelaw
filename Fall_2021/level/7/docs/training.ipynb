{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "## Are You in the Right Place?\n",
    "\n",
    "The following is part of a multi-part introduction to data science for those in the legal profession. The full collection of materials can be found at [https://www.codingthelaw.org](https://www.codingthelaw.org). This if from \n",
    "[Level 7](https://www.codingthelaw.org/level/7/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train a Model\n",
    "\n",
    "Warning! We are approaching the Danger Zone. That is, you're about to learn just enough to be dangerous. With great power comes great responsibility. \n",
    "\n",
    "![Data Science](http://static1.squarespace.com/static/5150aec6e4b0e340ec52710a/t/51525c33e4b0b3e0d10f77ab/1364352052403/Data_Science_VD.png?format=750w) \n",
    "\n",
    "Alright, let's go ahead and load Pandas. You may remeber this library from the data wrangeling notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data that we cleaned in as part of [Level 6](https://www.codingthelaw.org/level/6/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_table = pd.read_csv('processed_table_only_numbers.csv') \n",
    "processed_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "I'm now going to make a set of tables to be used in training and testing some models. This process should sound familiar from our prior discussions of training and testing. \n",
    "\n",
    "Remember, linear regression is used to predict continuous values (things that fall on a number line). I know we talked earlier (in the last notebook) about predicting snow days, but that's not a very good use case for linear regression. It's designed for predicting some number on a number line. So here I'm going to try to predict today's high using yesterday's. So I'll need the target (max) and the feature (yesterdays_high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lin_df = processed_table[[\n",
    "                               'max',\n",
    "                               'yesterdays_high'\n",
    "                               ]].copy()\n",
    "lin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we'd probably want to use more than one feature, but this one-to-one construction works well for an introduction. I susspect we'll see that yesterday's high is corelated with today's. That is, as one goes up, the other does the same. Let's see.\n",
    "\n",
    "But first, what about overfitting? Yep, I'm using a vocabulary word from the [reading](https://web.archive.org/web/20181204182136/https://lawyerist.com/big-bias-big-data/). To address that concern let's do a proper training of the data using training and holdout dataframes. That is, we'll set some amount of the data aside (holdout), train our model on what remains, and then check and see how the model does on the data it hasn't seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"size of all data\",len(lin_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe containing a random sample of lin_df rows\n",
    "lin_holdout = lin_df.sample(frac=0.20) # 0.20 = 20%\n",
    "\n",
    "# create a dataframe that conatins the rows from lin_df except for those in lin_holdout\n",
    "lin_training = lin_df.loc[~lin_df.index.isin(lin_holdout.index)]\n",
    "\n",
    "print(\"size of training\",len(lin_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's load some librarys and perform a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from statsmodels.graphics.regressionplots import abline_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model = ols(\"max ~ yesterdays_high\", lin_training).fit()\n",
    "\n",
    "# scatter-plot data\n",
    "ax = lin_training.plot(x='yesterdays_high', y='max', kind='scatter')\n",
    "\n",
    "# plot regression line\n",
    "fig = abline_plot(model_results=model, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I like the statsmodels regression library because it comes with a nice summary.\n",
    "# So let's load the library and run the regression on our training data. \n",
    "# There are two main numbers we should focus on below, the R-squared and the P-values. \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have yet to check the model against our holdout data, and it so happens that another implementation of the linear regression algo can be found over at scikitlearn. It makes evaluating against the holdout data rather simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rerun with SciKitLearn because it's easy to check accuracy\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "# make a training dataframe containing just features\n",
    "features_train = lin_training.drop(\"max\", axis=1).values\n",
    "\n",
    "# make a training dataframe containing only the target\n",
    "target_train = lin_training[\"max\"].values\n",
    "\n",
    "# make a testing/holdout dataframe containing just features\n",
    "features_test = lin_holdout.drop(\"max\", axis=1).values\n",
    "\n",
    "# make a testing/holdout dataframe containing only the target\n",
    "target_test = lin_holdout[\"max\"].values\n",
    "\n",
    "# run the regression\n",
    "lm = linear_model.LinearRegression()\n",
    "reg = lm.fit(features_train, target_train)\n",
    "print(\"R squared (training):\",lm.score(features_train,target_train)) # this sould be the same as above\n",
    "\n",
    "# create a dataframe of predictions based on the test features\n",
    "pred = reg.predict(features_test)\n",
    "# compare these predictions to the actual values\n",
    "variance = metrics.r2_score(target_test, pred)\n",
    "print(\"Variance score (test):\",variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model \"explains\" about 70% of the variance for both our training and holdout data. It turns out the temperature yesterday is a pretty good predictor for the temperature today. Imagine how good we'd be at prediction if we added a few other relevant features or if we lived in California (I'm told the weather is always the same ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "Now let's get back to predicting snow days. To predict a snow day we need a model that spits out a categorical value (i.e., a label like yes or no). This differs from a continuous value, like those we were looking at above. To make things simple, we're going to have yes = 1 and no = 0. That being said, let's clean up our processed table accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: the question of what features to include is beyond the scope of this post. \n",
    "# We'll just say these all look like they could help predict a snow day.\n",
    "\n",
    "class_df = processed_table[[\n",
    "                               'accum', \n",
    "                               'min', \n",
    "                               'max',\n",
    "                               'wind',\n",
    "                               #'yesterdays_high',\n",
    "                               'closed'\n",
    "                               ]].copy()\n",
    "class_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a little function to help evaluate how good our classifiers are. It produces the terms for a confusion matrix. See https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I'll need these libraries to make it work\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def evaluate(prob, pred, labels_test, verbose=1):\n",
    "    acc = accuracy_score(labels_test,pred)\n",
    "    ones = sum(labels_test)/len(labels_test)\n",
    "    zeros = (1 - ones)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels_test, pred).ravel()\n",
    "\n",
    "    recall_ = tp / (tp + fn) \n",
    "    precision_ = tp / (tp + fp)\n",
    "    f1 = (2 / ((1/recall_)+(1/precision_)))\n",
    "\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(labels_test,prob)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    if verbose==1:\n",
    "        \n",
    "        plt.rcParams['figure.figsize'] = [14, 4]\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "        label='AUC = %0.2f'% roc_auc)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.plot([0,1],[0,1],'r--')\n",
    "        plt.xlim([-0.1,1.2])\n",
    "        plt.ylim([-0.1,1.2])\n",
    "        plt.ylabel('True Positive (Hit) Rate')\n",
    "        plt.xlabel('False Positive (False Alarm) Rate')\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(labels_test,prob)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.plot(recall, precision,'b',\n",
    "        label='AP = %0.2f'% precision.mean())\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlim([-0.1,1.2])\n",
    "        plt.ylim([-0.1,1.2])\n",
    "        plt.ylabel('Recall')\n",
    "        plt.xlabel('Precision')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.hist([labels_test,prob], label=[\"true\",\"pred\"], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.title(\"Predictions vs Reality\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlim([-0.1,1.2])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()    \n",
    "\n",
    "        print (\"\\n0s: %0.2f\\tTrue Positives: %s\\tAccuracy: %s \"%(zeros,tp,acc))\n",
    "        print (\"1s: %0.2f\\tTrue Negatives: %s\\tAUC: %s\"%(ones,tn,roc_auc))\n",
    "        print (\"\\t\\tFalse Positives: %s\\tF1 Score: %s\"%(fp,f1))\n",
    "        print (\"\\t\\tFalse Negatives: %s\\tRecall (fract of actual yeas found): %s\"%(fn,recall_))\n",
    "        print (\"\\t\\t\\t\\t\\tPrecision (correctness of yeas predicted): %s\\n\"%precision_)        \n",
    "        \n",
    "        null = zeros\n",
    "        if null < ones:\n",
    "            null = ones\n",
    "        \n",
    "        if (acc>null) and (roc_auc>0.5) and (recall_>0.5) and (precision_>0.5):\n",
    "            print(\"################\")\n",
    "            print(\"#     PASS     #\")\n",
    "            print(\"################\")\n",
    "            vpass = \"Yes\"\n",
    "        else:\n",
    "            print(\"################\")\n",
    "            print(\"#     FAIL     #\")\n",
    "            print(\"################\")\n",
    "            vpass = \"No\"\n",
    "          \n",
    "    #return ones,vpass,null,acc,roc_auc,recall_,precision_,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course we need training and holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a dataframe containing a random sample of rows\n",
    "class_holdout = class_df.sample(frac=0.20)\n",
    "\n",
    "# create a dataframe that conatins the rows from except for those in holdout\n",
    "class_training = class_df.loc[~class_df.index.isin(class_holdout.index)]\n",
    "\n",
    "# make a training dataframe containing just features\n",
    "features_train = class_training.drop(\"closed\", axis=1).values\n",
    "\n",
    "# make a training dataframe containing only the target\n",
    "labels_train = class_training[\"closed\"].values\n",
    "\n",
    "# make a testing/holdout dataframe containing just features\n",
    "features_test = class_holdout.drop(\"closed\", axis=1).values\n",
    "\n",
    "# make a testing/holdout dataframe containing only the target\n",
    "labels_test = class_holdout[\"closed\"].values\n",
    "\n",
    "print(\"size of training\",len(class_training))\n",
    "\n",
    "# Show how many instances there are of closed (1) and opened (0)\n",
    "print(class_training[\"closed\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just to keep ourselves honest, let's ask, \"What percentage of the time is target 0?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of 0s: %s\\n\"%(len(class_df[class_df[\"closed\"]==0])/len(class_df)))\n",
    "\n",
    "# This is a useful question because it helps us not be overly impressed by the accuracy of our models. \n",
    "# How? Well, apparently, if I always guessed that we wouldn't have a snow day I'd be right ~98% of the time.\n",
    "\n",
    "print(class_df[\"closed\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is the feeding or our data to a bunch of different classification algorithms. The \"best\" models will have the highest F1 score. At least, that's how we'll define best at the moment. \n",
    "\n",
    "Note, they may or may not work depending on the contents of your training data. Since there are so few school closings, it's possible you won't get enough of them in the training data for the algos to work. When this happens you'll likely see a pink error message with something like \"divide by zero encountered.\" If this happens and you really want the pink to go away, rerun the cell above where you create the training and holdout dataframes. When you use bigger datasets this shouldn't be an issue. \n",
    "\n",
    "I've provided links to documentation on each of the classification algos so you can investigate what they're doing under the hood. That being said, this documentation is probably not the best introduction. So you may want to consider Googling their names.\n",
    "\n",
    "First up, \n",
    "## Logistic Regression \n",
    "http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept = False, C = 1e9)\n",
    "clf_1 = model.fit(features_train, labels_train)\n",
    "pred = clf_1.predict(features_test)\n",
    "prob = clf_1.predict_proba(features_test)[:,1] \n",
    "print(\"Logistic Regression\")\n",
    "evaluate(prob, pred, labels_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "http://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf_2 = tree.DecisionTreeClassifier(random_state=0,min_samples_split=2,max_leaf_nodes=2,min_samples_leaf=1)\n",
    "clf_2 = clf_2.fit(features_train, labels_train)\n",
    "\n",
    "print(\"Decsion Tree\")\n",
    "\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf_2, out_file=None, \n",
    "                      feature_names=[\"accumulation\" ,\"min temp\",\"max temp\",\"wind\"],  \n",
    "                      class_names=[\"open\",\"closed\"],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = clf_2.predict(features_test)\n",
    "prob = clf_2.predict_proba(features_test)[:,1] \n",
    "print(\"Decision Tree\")\n",
    "evaluate(prob, pred, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "http://scikit-learn.org/stable/modules/ensemble.html#forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_3 = RandomForestClassifier()\n",
    "clf_3 = clf_3.fit(features_train, labels_train)\n",
    "pred = clf_3.predict(features_test)\n",
    "prob = clf_3.predict_proba(features_test)[:,1] \n",
    "print(\"Random Forest\")\n",
    "evaluate(prob,pred, labels_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions \n",
    "To actually use your models we use `.predict()`. All you do is pass your model a list of feature values, and it will spit out its best guess re: answers. You can actually pass a multi-dimensional array of values (basically a table), but here we'll limit ourselves to a simple list. The ability to pass an array however, is why you see bracketed numbers after `.predict()`. They're asking that the output only focus on the bits we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What classifier do you want to use? Above, each one is \n",
    "# given it's own number (e.g., clf_1, clf_2, etc.)\n",
    "clf = clf_2\n",
    "\n",
    "# Set the values for each of the above features: accum, min, max, wind\n",
    "inputs = [30,-20,50,50]\n",
    "\n",
    "guess = clf.predict([inputs])[0]\n",
    "print(\"Model's Guess:\",guess)\n",
    "\n",
    "if guess == 0:\n",
    "    probability = clf.predict_proba([inputs])[0][0]\n",
    "else:\n",
    "    probability = clf.predict_proba([inputs])[0][1]\n",
    "    \n",
    "print(\"Confidence the guess is right:\",probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now back to that multi-dimensional array. `features_test` is just such an array. So `pred_[number]` is just a list of numbers (1= snow day and 0= no snow day) for that list. Here's the list of features (a list of lists with three numbers per \"row\"):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_test[:3] # the [:3] is just a way to say, show only the first three \"rows\", kind of like .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have guessed it, but an array is just another datatype, it's kind of like a dataframe but it's a list of lists where the lists are numbers. Anywho. You can feed this array into your model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as you can see each of the numbers in `pred` corresponds to either a 1 (snow day) or a 0 (no snow day).\n",
    "\n",
    "Let's say that for example, I wanted to take the data from above and see for what accumulations the algo predicted school would be closed. Then I could pass it our feature data (`features_test`), marry it to our acummulation data, and filter by `1`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_test[:,0] # This is just the way to get the first 0ith column of data (accum) in the feature test array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I can produce a similar array of accumulations and marry the two together into a dataframe where I can filter out predictions of `0`. Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentile_list = pd.DataFrame(\n",
    "    {'accum': features_test[:,0], 'pred': clf.predict(features_test)\n",
    "    })\n",
    "percentile_list[percentile_list[\"pred\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProTip: This only works because the order of the accumulation list and the prediction list are the same. If I wanted to do something like figure out the dates on which the algo predicted a closing, I would need a list with the same order. This means you have to think about when you get rid of columns because you might want them later on. \n",
    "\n",
    "And if you want to get the contents of a column in a nice comma seperated list, just add `.tolist()` to the end of it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentile_list[percentile_list[\"pred\"]==1][\"accum\"].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
